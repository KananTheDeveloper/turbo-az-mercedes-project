{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to turbo_az_mercedes.csv file\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import concurrent.futures\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define headers for the request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36',\n",
    "    'Accept-Language': 'en-US, en;q=0.5'\n",
    "}\n",
    "\n",
    "# Define the base URL\n",
    "baseurl = \"https://turbo.az\"\n",
    "\n",
    "conversion_rates = {\n",
    "    'USD': 1.70,  # USD to AZN\n",
    "    'EUR': 1.88   # EUR to AZN\n",
    "}\n",
    "\n",
    "# Function to fetch car links from a specific page\n",
    "def fetch_car_links(page_num):\n",
    "    r = requests.get(f\"{baseurl}/autos?page={page_num}&q%5Bmake%5D%5B%5D=4\", headers=headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    return [baseurl + item.find(\"a\", href=True)[\"href\"] for item in soup.find_all(\"div\", class_='products-i')]\n",
    "\n",
    "def fetch_car_details(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract car ID, set to 0 if not found\n",
    "    car_id = soup.find('div', class_='product-actions__id')\n",
    "    car_id = int(car_id.text.strip().replace(\"Elanın nömrəsi: \", \"\")) if car_id else None\n",
    "    \n",
    "    # Extracting and converting price\n",
    "    price = soup.find('div', class_=\"product-price__i\")\n",
    "    price_value = None\n",
    "    if price:\n",
    "        price_text = price.text.strip()\n",
    "        price_value = int(''.join(filter(str.isdigit, price_text)))\n",
    "        if \"USD\" in price_text:\n",
    "            price_value = round(price_value * conversion_rates['USD'])\n",
    "        elif \"EUR\" in price_text:\n",
    "            price_value = round(price_value * conversion_rates['EUR'])\n",
    "\n",
    "\n",
    "    # Initializing number_of_views to None\n",
    "    number_of_views = None\n",
    "\n",
    "    # Extracting number of views, set to 0 if not found\n",
    "    all_spans = soup.find_all('span', class_=\"product-statistics__i-text\")\n",
    "    for span in all_spans:\n",
    "        if \"Baxışların sayı\" in span.text:\n",
    "            number_of_views = int(span.text.strip().replace(\"Baxışların sayı: \", \"\"))\n",
    "            break\n",
    "\n",
    "    # Extract car properties\n",
    "    properties = {\n",
    "        'Marka': None,\n",
    "        'Model': None,\n",
    "        'Buraxılış ili': None,\n",
    "        'Şəhər': None,\n",
    "        'Rəng': None,\n",
    "        'Yürüş (km)': None,\n",
    "        'Mühərrik': None,\n",
    "        'At gücü': None,\n",
    "        'Benzin tipi': None,\n",
    "        'Sürətlər qutusu': None,\n",
    "        'Yeni': None,\n",
    "        'Vəziyyəti': None\n",
    "    }\n",
    "\n",
    "    for el in soup.find_all('div', {'class': 'product-properties__i'}):\n",
    "        label = el.find('label').text.strip()\n",
    "        value = el.find('span', class_='product-properties__i-value').text.strip()\n",
    "        \n",
    "        if label == 'Buraxılış ili':\n",
    "            properties[label] = int(value) if value else None\n",
    "        elif label == 'Yürüş':\n",
    "            properties['Yürüş (km)'] = int(''.join(filter(str.isdigit, value))) if value else None \n",
    "        elif label == 'Mühərrik':\n",
    "            engine_details = value.split('/')\n",
    "            properties['Mühərrik'] = float(engine_details[0].replace(\" L\", \"\")) if len(engine_details) > 0 else None\n",
    "            properties['At gücü'] = int(re.findall(r'\\d+', engine_details[1])[0]) if len(engine_details) > 1 else None\n",
    "            properties['Benzin tipi'] = engine_details[2] if len(engine_details) > 2 else None\n",
    "        else:\n",
    "            properties[label] = value if value else None\n",
    "\n",
    "\n",
    "    # Combine all details into a single dictionary\n",
    "    return {\n",
    "        'ID': car_id,\n",
    "        'Marka': properties['Marka'],\n",
    "        'Model': properties['Model'],\n",
    "        'Buraxılış ili': properties['Buraxılış ili'],\n",
    "        'Şəhər': properties['Şəhər'],\n",
    "        'Qiymət (AZN)': price_value,\n",
    "        'Rəng': properties['Rəng'],\n",
    "        'Yürüş (km)': properties['Yürüş (km)'],\n",
    "        'Mühərrik (L)': properties['Mühərrik'],\n",
    "        'At gücü (a.g)': properties['At gücü'],\n",
    "        'Benzin tipi': properties['Benzin tipi'],\n",
    "        'Sürətlər qutusu': properties['Sürətlər qutusu'],\n",
    "        'Yeni': properties['Yeni'],\n",
    "        'Vəziyyəti': properties['Vəziyyəti'],\n",
    "        'Baxışların sayı': number_of_views\n",
    "    }\n",
    "\n",
    "# Main script to collect and save car data\n",
    "if __name__ == \"__main__\":\n",
    "    productlinks = []\n",
    "    max_workers = 10  # Number of threads for parallel processing\n",
    "\n",
    "    # Use concurrent.futures to fetch car links in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Adjust range for the number of pages you want to scrape\n",
    "        future_to_page = {executor.submit(fetch_car_links, page_num): page_num for page_num in range(1,316)}\n",
    "        for future in concurrent.futures.as_completed(future_to_page):\n",
    "            productlinks.extend(future.result())\n",
    "\n",
    "    # Use concurrent.futures to fetch car details in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {executor.submit(fetch_car_details, url): url for url in productlinks}\n",
    "        car_results = [future.result() for future in concurrent.futures.as_completed(future_to_url)]\n",
    "    \n",
    "    # Convert the collected data into a DataFrame\n",
    "    cars_df = pd.DataFrame(car_results)\n",
    "    \n",
    "    # Write the data to a CSV file\n",
    "    cars_df.to_csv(\"scraped_mercedes_data.csv\", index=False, encoding='utf-8')\n",
    "    \n",
    "    print(\"Data has been written to scraped_mercedes_data.csv file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
